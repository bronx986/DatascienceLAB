# -*- coding: utf-8 -*-
"""Laboratorio_data_analisi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWKN9xisGMRYTW_zcMjQwOoX-Op0aNgb

Codice Tsne Carmine e David
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Dataset di partenza : https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
# import dataset csv
name = '/content/healthcare-dataset-stroke-data.csv'

# construct data frame
df = pd.read_csv (name)


#######   early EDA: part1  ############
cols = df.columns
summary = df.describe(include='all')

print('\n#########  columns name:\n')
print(cols)
print('\n#########  short info columns data type:\n')
print(df.info())
print('\n#########  summary info columns distribution:\n')
print(summary)

#######   early EDA: part 2  ############

# focus on specific features:
print("\n ### smoking status label:\n")
print(df['smoking_status'].value_counts())

print("\n ### gender label:\n")
print(df['gender'].value_counts())

print("\n ### work_type label:\n")
print(df['work_type'].value_counts())

# since age range lower limit include pediatric age (0-18 years) and
# "strokes belong to the rare conditions in the pediatric population"
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6026646/
# find how many stroke record are present in this age range

print("\n ### pediatric age records:\n")
print(df[df['age'] < 18].shape[0])

print("\n ### pediatric age strokes:\n")
print(df[(df['age'] < 18) & (df['stroke'] == 1) ].shape[0])

print("\n ### pediatric age strokes - complete features records:\n")
print(df[(df['work_type'] == "children") & (df['stroke'] == 1) ])

print("\n ### smoking_status = Unknown in pediatric age:\n")
print(df[(df['age'] < 18) & (df['smoking_status'] == "Unknown")].shape[0])

print("\n ### smoking_status = Unknown + working_type = children -> in pediatric age:\n")
print(df[(df['age'] < 18) & (df['smoking_status'] != "Unknown") & (df['work_type'] == "children")].shape[0])

print("\n ### smoking_status != Unknown -> in pediatric age:\n")
print(df[(df['age'] < 18) & (df['smoking_status'] != "Unknown")].value_counts('smoking_status'))

print("\n ### bmi = NaN -> in pediatric age:\n")
print(df[(df['age'] < 18) & (df['bmi'].isna())].shape[0])

#######   early EDA: summary and conclusions  ############

# First step for a dataset improvement resides into a modification along age:
# there is a sharp medical and socio-cultural separation bewteen
# pediatric (<18 years) and non pediatric age (>18 years).
# Although stroke could happen in pediatric age this has to be considered like
# a rare event linked prevalentely to pediatric phisiology and not affected by
# lifestyle factors like job, food intake, family and smoking behaviour.
# These lifestyle factors, indeed, carriy their effect in the long term
# of adult age. Thus it is a different picture from the dataset context
# description from Kaggle "According to the World Health Organization (WHO)
# stroke is the 2nd leading cause of death globally, responsible for
# approximately 11% of total deaths.

# Ref for pediatric phisiology differences:
# https://www.chop.edu/conditions-diseases/pediatric-stroke
# https://www.monarquehealth.com/blog/how-is-pediatric-care-different-from-working-with-adults
# https://www.cdc.gov/childrenindisasters/differences.html


# Second step for a dataset improvement is considering "bmi == NaN" and
# "smoking_status = Unknown" as suitable targets for a data cleaning
# as they represent lack of informations. For "smoking_status = Unknown" half
# of records belong to pediatric age, instead only 20 over circa 200 records
# of "bmi == NaN" belong to it.

# Third step is to evaluate gender label "Other" along "Female" and "Male" as not
# informative about the leading body phisiology or altered body phisiology
# (i.e. hormone therapy) that could be linked to strokes.
# Morevore "Other" is present in dataset as only 1 unit, therefore
# could be easely as a datset outlier.

#######   DATA CLEANING  ############

# 1. get rid of pediatric (<18 years) records
# 2. get rids of gender 'Other' (only 1 non binary Other)

df = df.loc[df['age'] >= 18]
df = df.loc[df['gender'] != "Other"]

print("\n ### entries after data cleaning 1 + 2 :\n")
print(df.shape[0])

# 3. action on smoking status (Unkwon not informative), gender (only 1 non binary Other)

df = df.loc[df['smoking_status'] != "Unknown"]

print("\n ### entries after data cleaning 3 :\n")
print(df.shape[0])

# 4. action on bmi with 4909 over 5110 entries missing NA data in original dataset

df = df.dropna(subset=['bmi'])

print("\n ### entries after data cleaning 4 :\n")
print(df.shape[0])

#######   mid EDA: part 1  ############

### Check target labels balance:
print("\n ########  Target Labels Balance:  ########  \n")
print(f"\n {df['stroke'].value_counts()}")
print(f"\nratio stroke/no stroke:\n {round(df['stroke'].value_counts()[1]/df['stroke'].value_counts()[0], 2)}")

df.info()

### Transform categorical data in continuos data:

from sklearn.preprocessing import OneHotEncoder
import pandas as pd


# Inizializzazione dell'encoder
encoder = OneHotEncoder(sparse=False)

# Lista diretta delle colonne categoriche
categorical_columns = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Verifica che tutte le colonne selezionate esistano nel DataFrame
if not all(column in df.columns for column in categorical_columns):
    missing = [column for column in categorical_columns if column not in df.columns]
    raise ValueError(f"Le colonne {missing} non sono presenti nel DataFrame.")

# Adattare e trasformare i dati categorici
encoded_data = encoder.fit_transform(df[categorical_columns])

# Usa `get_feature_names_out` per le versioni piÃ¹ recenti di scikit-learn
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenare con le colonne non categoriche
final_df = pd.concat([encoded_df, df.drop(columns=categorical_columns)], axis=1)
final_df = final_df.drop(columns=['id'])

print("\n ########  Dataframe info after One Hot Encoding:  ######## \n")
final_df.info()

# eliminazione NaN prima di usare tsne
final_df = final_df.dropna()

print("\n ########  Dataframe info after NA removal:  ######## \n")
final_df.info()

### Check again target labels balance:

print("\n ########  Target Labels Balance:  ########  \n")
print(f"\n {final_df['stroke'].value_counts()}")
print(f"\nratio stroke/no stroke:\n {round(final_df['stroke'].value_counts()[1]/final_df['stroke'].value_counts()[0], 2)}")

### Correlation 1

### Try a Pearson correlation to verify which features show a real connection
### with 'stroke' target label:

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(13,8))
# define the mask to set the values in the upper triangle to True
mask = np.triu(np.ones_like(final_df.corr(), dtype=bool))
heatmap = sns.heatmap(final_df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':16}, pad=16);

### t-SNE 1

### Try a t-SNE to verify cluster:

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, random_state=42)

# apply t-SNE on vectorialized data:
tsne_results = tsne.fit_transform(final_df.iloc[:, 12:17])

# plot 2D embedding
plt.figure(figsize=(8, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=final_df.iloc[:,-1])
plt.title('t-SNE visualization')
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

#####   DATASET BALANCING  ##########

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
import numpy as np

#### Define your features (X) and target (y)

# features could be passed in dataframe format
X = final_df.iloc[:, 0:18]

# target labels has to be passed as numpy array
y = final_df['stroke'].astype('category')
y = y.to_numpy()

print('\n###  Original dataset shape: %s\n' % Counter(y))

# appply SMOTE oversampling technique to balance dataset:
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)

print('\n###  Resampled dataset shape: %s\n' % Counter(y_res))

### Correlation 2

### Try a Pearson correlation to verify which features show a real connection
### with 'stroke' target label:

import matplotlib.pyplot as plt
import numpy as np

final_df_sm = X_res.assign(stroke=y_res)

plt.figure(figsize=(13,8))
# define the mask to set the values in the upper triangle to True
mask = np.triu(np.ones_like(final_df_sm.corr(), dtype=bool))
heatmap = sns.heatmap(final_df_sm.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Triangle Correlation Heatmap - after target label balancing', fontdict={'fontsize':16}, pad=16);

### FEATURE ENGINEEARING

# Crete a feature to express probable Metabolic Syndrome named 'MetS':
# metabolic syndrome is significantly associated with both stroke recurrence and all-cause mortality.
# Overweight (bmi >27) and diabete (avg glucose level > 140) are signature feature of MetS. Along
# this two, other conditions like high colesterol work towards a bad cardiocirculatory state.

import pandas as pd
import numpy as np
final_df_sm['MetS'] = np.where((final_df_sm['avg_glucose_level'] > 140) & (final_df_sm['bmi'] > 27), 1, 0)

### Correlation 3

### Try a Pearson correlation to verify which features show a real connection
### with 'stroke' target label:

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(13,8))
# define the mask to set the values in the upper triangle to True
mask = np.triu(np.ones_like(final_df_sm.corr(), dtype=bool))
heatmap = sns.heatmap(final_df_sm.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Triangle Correlation Heatmap - after adding MetS feature', fontdict={'fontsize':16}, pad=16);

### t-SNE 2

### Try a t-SNE to verify cluster:

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, random_state=42)

# apply t-SNE on vectorialized data:
tsne_results = tsne.fit_transform(X_res.iloc[:,0:20])

# plot 2D embeddingplt.figure(figsize=(8, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=y_res)
plt.title('t-SNE visualization')
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

# Applicare t-SNE ai dati vettorizzati
tsne_results = tsne.fit_transform(final_df.iloc[:, 0:17])

# plot
plt.figure(figsize=(8, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=final_df.iloc[:,-1])
plt.title('t-SNE visualization')
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

############# from here code not updated on 17/3/2024

# cerchiamo dei gruppi in questo dataset
from sklearn.cluster import KMeans
import numpy as np

# Applicazione di K-Means sui risultati t-SNE
n_clusters =2  # Sostituisci con il numero ottimale di cluster che desideri trovare
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(tsne_results)

# Aggiungere l'assegnazione del cluster al DataFrame
final_df['Cluster'] = clusters
import matplotlib.pyplot as plt
import seaborn as sns

# Visualizzazione dei cluster t-SNE
plt.figure(figsize=(10, 8))
sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=clusters, palette='viridis', s=50, alpha=0.6)
plt.title('t-SNE con Cluster K-Means')
plt.xlabel('Componente t-SNE 1')
plt.ylabel('Componente t-SNE 2')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Calcolare la media per le variabili numeriche in ciascun cluster
cluster_summary = final_df.groupby('Cluster').mean().reset_index()

# Impostare il numero di colonne per i subplot
n_cols = 3  # Numero di colonne per i subplot
n_rows = (len(cluster_summary.columns) - 1 + n_cols - 1) // n_cols  # Calcola il numero di righe necessarie

# Impostare la palette di colori in base al numero di cluster
palette = sns.color_palette("viridis", n_clusters)

# Inizializzare la figura
plt.figure(figsize=(n_cols * 6, n_rows * 5))  # Modifica le dimensioni se necessario

for i, column in enumerate(cluster_summary.columns[1:]):  # Esclude 'Cluster'
    plt.subplot(n_rows, n_cols, i + 1)  # Usa n_rows e n_cols qui
    sns.barplot(x='Cluster', y=column, data=cluster_summary, palette=palette)
    plt.title(f'Distribuzione di {column} per Cluster')
    plt.ylabel('Media')
    plt.xlabel('Cluster')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(8,8))
# define the mask to set the values in the upper triangle to True
mask = np.triu(np.ones_like(df.corr(), dtype=bool))
heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);
df.corr()

import seaborn as sns
sns.set_theme(style="ticks")
plt.figure(figsize=(8,8))
sns.pairplot(df, hue="stroke")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# Assuming 'df' is your DataFrame
df.info()
# Drop the 'id' column
df = df.drop(columns=['id'])

# Identify the categorical columns by name
categorical_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Define the transformer for categorical columns
categorical_transformer = OneHotEncoder()

# Create the preprocessing pipeline with one-hot encoding for categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough'  # This will keep all other features untouched
)

df.corr()

# Define your features (X) and target (y)
X = df.drop('stroke', axis=1)
y = df['stroke']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ###################################### OVER-SAMPLING:
# https://imbalanced-learn.org/stable/over_sampling.html

# from imblearn.over_sampling import RandomOverSampler
# ros = RandomOverSampler(random_state=0)

# X_ros, y_ros = ros.fit_resample(X, y)

# # Split the data into training and test sets
# X_ros_train, X_ros_test, y_ros_train, y_ros_test = train_test_split(X_ros, y_ros, test_size=0.2, random_state=42)

# ############################################################


# Create the full pipeline with preprocessing and the classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
predictions = pipeline.predict(X_test)

# train set target label balance (0.044 for dataset):
y_train.value_counts()[1]/y_train.value_counts()[0]

# test set target label balance (0.044 for dataset):
y_test.value_counts()[1]/y_test.value_counts()[0]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Evaluate on the training set
train_predictions = pipeline.predict(X_train)
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_train, train_predictions):.4f}")
print(f"Precision: {precision_score(y_train, train_predictions):.4f}")
print(f"Recall: {recall_score(y_train, train_predictions):.4f}")
print(f"F1 Score: {f1_score(y_train, train_predictions):.4f}")
train_probs = pipeline.predict_proba(X_train)[:, 1]  # Probabilities for the positive class
print(f"ROC AUC: {roc_auc_score(y_train, train_probs):.4f}")

# Evaluate on the test set
print("\nTest Metrics:")
test_predictions = pipeline.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, test_predictions):.4f}")
print(f"Precision: {precision_score(y_test, test_predictions):.4f}")
print(f"Recall: {recall_score(y_test, test_predictions):.4f}")
print(f"F1 Score: {f1_score(y_test, test_predictions):.4f}")
test_probs = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for the positive class
print(f"ROC AUC: {roc_auc_score(y_test, test_probs):.4f}")

import numpy as np

# Train the model as before
pipeline.fit(X_train, y_train)

# Get the feature importances
importances = pipeline.named_steps['classifier'].feature_importances_

# Get the names of the one-hot encoded features
encoder = pipeline.named_steps['preprocessor'].named_transformers_['cat']
encoded_features = encoder.get_feature_names_out(categorical_features)

# Combine the feature names from one-hot encoding and the remainder of the columns
all_features = np.concatenate([encoded_features, X_train.columns.difference(categorical_features)])

# Create a DataFrame to display feature importances
feature_importances = pd.DataFrame({
    'Feature': all_features,
    'Importance': importances
})

# Sort the DataFrame to show the most important features at the top
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

print(feature_importances)

import matplotlib.pyplot as plt

# Number of features to plot (you can change this to your preference or use len(feature_importances) for all)
num_features = len(feature_importances)

# Sort features according to importance
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(10, 10))
plt.barh(feature_importances['Feature'][:num_features], feature_importances['Importance'][:num_features])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()  # To display the most important feature at the top
plt.title('Feature Importances')

plt.show()
